# -*- coding: utf-8 -*-
"""DeepByWrite_triple_lossAUC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I7zPDS2MwwarF2mI8-qjLdtLCEUxhGFM
"""

pip install ml_metrics

import os
import csv
import ml_metrics
import pandas as pd
from skimage import io
import torch
import torch.nn as nn
from torchvision import transforms
from torch.utils.data import Dataset
from torchvision.models import resnet34
from torch.autograd import Function
from torch.nn.modules.distance import PairwiseDistance
import torch.optim as optim
from torch.optim import lr_scheduler
import operator
import numpy as np
from sklearn.model_selection import KFold
from scipy import interpolate

import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

class TripletDataset(Dataset):
  def __init__(self,root_dir,csv_name, num_triplets,transform= None):
    self.root_dir = root_dir
    self.df =   pd.read_csv(csv_name)#dataframe
    self.num_triplets = num_triplets
    self.transform = transform
    self.training_triplets = self.generate_triplets(self.df,self.num_triplets)
    
  @staticmethod
  def generate_triplets(df, num_triplets):
    def make_dictionary_for_letter_class(df):
      letter_classes = dict()
      for idx, label in enumerate(df['class']):
        if label not in letter_classes: 
          letter_classes[label]= []

        letter_classes[label].append(df['id'][idx])
      return letter_classes 
    triplets = []
    classes = df['class'].unique()
    
    letter_classes = make_dictionary_for_letter_class(df)

    for _ in range (num_triplets):
     #randomly choose a,pn
      pos_class = np.random.choice(classes)
      neg_class = np.random.choice(classes)
      while len(letter_classes[pos_class])<2:
        pos_class = np.random.choice(classes)
      while pos_class == neg_class:
        neg_class = np.random.choice(classes)
      pos_name = df.loc[df['class']== pos_class,'name'].values[0]
      neg_name = df.loc[df['class']== neg_class,'name'].values[0]
      
      if len(letter_classes[pos_class])==2:
        id_name_anc,id_name_pos = np.random.choice(letter_classes[pos_class],2) # image anchor image positive
      else:
        id_name_anc = np.random.choice(letter_classes[pos_class])
        id_name_pos = np.random.choice(letter_classes[pos_class])
        while id_name_anc == id_name_pos:
          id_name_pos = np.random.choice(letter_classes[pos_class])
          
      id_name_neg = np.random.choice(letter_classes[neg_class])
      #print(80*'=')
      #print (letter_classes[pos_class])
      #print(80*'=')
      #print (letter_classes[neg_class])
      #print(80*'=')
      #print(id_name_anc,id_name_pos,id_name_neg)
      #break
      #try:
      #  id_name_anc=df.loc[df['class']== ianc,'id'].values[0] 
      # id_name_pos=df.loc[df['class']== ipos,'id'].values[0]
      #  id_name_neg=df.loc[df['class']== ineg,'id'].values[0]
      #  triplets.append([id_name_anc,id_name_pos,id_name_neg,pos_class,neg_class,pos_name,neg_name])
        
      #except:
      #  continue
      #triplets.append([letter_classes[pos_class][ianc],letter_classes[pos_class][ipos],letter_classes[neg_class][ineg],pos_class,neg_class,pos_name,neg_name])
      #print(id_name_neg)
      
      triplets.append([id_name_anc,id_name_pos,id_name_neg,pos_class,neg_class,pos_name,neg_name])
    return triplets

  def __getitem__(self,idx):
    anc_id,pos_id,neg_id,pos_class,neg_class, pos_name,neg_name = self.training_triplets[idx]
    anc_img=os.path.join(self.root_dir,str(pos_name),str(anc_id)+'.png')
    pos_img=os.path.join(self.root_dir,str(pos_name),str(pos_id)+'.png')
    neg_img=os.path.join(self.root_dir,str(neg_name),str(neg_id)+'.png')
    #print(anc_img,pos_img,neg_img)
    anc_img=io.imread(anc_img) 
    pos_img=io.imread(pos_img)
    neg_img=io.imread(neg_img)

    pos_class=torch.from_numpy(np.array([pos_class]).astype('long'))
    neg_class=torch.from_numpy(np.array([neg_class]).astype('long'))

    sample = {'anc_img':anc_img,'pos_img':pos_img,'neg_img':neg_img,'pos_class':pos_class,'neg_class':neg_class}

    if self.transform:
      sample['anc_img'] = self.transform(sample['anc_img'])
      sample['pos_img'] = self.transform(sample['pos_img'])
      sample['neg_img'] = self.transform(sample['neg_img'])
    return sample

  def __len__(self):
    return len(self.training_triplets)

def get_dataloader(train_root_dir,valid_root_dir, train_csv_name,valid_csv_name, num_train_triplets, num_valid_triplets, batch_size,num_workers):
  data_transforms = {
        'train': transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])]),
        'valid': transforms.Compose([
            transforms.ToPILImage(),
            transforms.ToTensor(),
            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])])}

  letter_dataset = {
        'train' : TripletDataset(root_dir     = train_root_dir,
                                     csv_name     = train_csv_name,
                                     num_triplets = num_train_triplets,
                                     transform    = data_transforms['train']),
        'valid' : TripletDataset(root_dir     = valid_root_dir,
                                     csv_name     = valid_csv_name,
                                     num_triplets = num_valid_triplets,
                                     transform    = data_transforms['valid'])}

  dataloaders = {
        x: torch.utils.data.DataLoader(letter_dataset[x], batch_size = batch_size, shuffle = False, num_workers = num_workers)
        for x in ['train', 'valid']}
    
  #data_size = {'train':len(os.listdir(train_root_dir)), 'valid':len(os.listdir(valid_root_dir))}
  data_size = {x: len(letter_dataset[x]) for x in ['train', 'valid']}   
 
  return dataloaders, data_size

class LetterNetModel(nn.Module):
  def __init__(self, embedding_size,num_classes,pretrained=False):
    super(LetterNetModel, self).__init__()
    self.model = resnet34(pretrained)
    self.embedding_size = embedding_size
    self.model.fc = nn.Linear(2048*2*2,self.embedding_size)
    self.model.classifier = nn.Linear(self.embedding_size,num_classes)
  def l2_norm(self,input):
    input_size= input.size()
    buffer = torch.pow(input,2)
    normp =torch.sum(buffer,1).add_(1e-10)
    _output = torch.sqrt(normp)
    output = _output.view(input_size)
    return output
  def forward(self,x):
    x = self.model.conv1(x)
    x = self.model.bn1(x)
    x = self.model.relu(x)
    x = self.model.maxpool(x)
    x = self.model.layer1(x)
    x = self.model.layer2(x)
    x = self.model.layer3(x)
    x = self.model.layer4(x)
    
    x = x.view(x.size(0), -1)
    
    #print(x.data.size())
    x = self.model.fc(x)
    #(H,W)=x.data.size()
    #x = x.view(H*W,-1)
    self.features = x
    # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf
    #alpha         = 10
    #self.features = self.features*alpha

    return self.features

  def forward_classifier(self, x):
    features = self.forward(x)
    res= self.model.classifier(features)
    return res

class TripletLoss(Function):
  def __init__(self,margin):
    super (TripletLoss,self).__init__()
    self.margin= margin
    self.pdist= PairwiseDistance(2)
  def forward(self,anchor,positive,negative):
    pos_dist = self.pdist.forward(anchor, positive)
    neg_dist = self.pdist.forward(anchor,negative)
    
    hinge_dist = torch.clamp(self.margin+pos_dist-neg_dist,min=0.0)
    loss = torch.mean(hinge_dist)
    return loss

predicted_list = []
actual_list = []

def calculate_accuracy(threshold,dist,actual_issame):
  predict_issame = np.less(dist,threshold)
  
  tp = np.sum(np.logical_and(predict_issame, actual_issame))
  fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))
  tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))
  fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))
  
  tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn) # recall
  fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn) # false positve rate
  pres= 0 if (tp+fp==0) else float(tp) / float(tp+fp) # precision
  
  
  acc = float(tp+tn)/dist.size
  return tpr, fpr, acc,pres

def calculate_accuracy_test(threshold,dist,actual_issame):
  predict_issame = np.less(dist,threshold)
  predicted_list.append(predict_issame.astype(float))
  actual_list.append(actual_issame)
  tp = np.sum(np.logical_and(predict_issame, actual_issame))
  fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))
  tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))
  fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))
  
  tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn) # recall
  fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn) # false positve rate
  pres= 0 if (tp+fp==0) else float(tp) / float(tp+fp) # precision
  
  
  acc = float(tp+tn)/dist.size
  return tpr, fpr, acc,pres

def calculate_roc(thresholds,distances,labels,nrof_folds=5):
  nrof_pairs =min(len(labels),len(distances)) 
  print('========NUMBER OF PAIRS ======',len(labels),len(distances))
  nrof_thresholds =len(thresholds)
  k_fold = KFold(n_splits = nrof_folds,shuffle=False)
  tprs= np.zeros((nrof_folds,nrof_thresholds))
  fprs= np.zeros((nrof_folds,nrof_thresholds))
  
  presss = np.zeros((nrof_folds,nrof_thresholds))
  accuracy = np.zeros((nrof_folds))
  
  indices = np.arange(nrof_pairs)
  for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):
    #print(train_set,test_set)
    acc_train = np.zeros((nrof_thresholds))
    for threshold_idx,threshold in enumerate(thresholds):
      _,_,acc_train[threshold_idx],_ = calculate_accuracy(threshold,distances[train_set],labels[train_set])
    best_threshold_index = np.argmax(acc_train)
    for threshold_idx, threshold in enumerate(thresholds):
      tprs[fold_idx,threshold_idx],fprs[fold_idx,threshold_idx],_,presss[fold_idx,threshold_idx] = calculate_accuracy(threshold,distances[test_set],labels[test_set])
    _,_,accuracy[fold_idx],_ = calculate_accuracy_test(thresholds[best_threshold_index],distances[test_set],labels[test_set])
    tpr = np.mean(tprs,0)
    fpr = np.mean(fprs,0)
    pres = np.mean(presss,0)
  return tpr,fpr,accuracy,pres

def calculate_val(thresholds,distances,labels,far_target=1e-3,nrof_folds=5):
  nrof_pairs = min(len(labels),len(distances))
  nrof_thresholds = len(thresholds)
  k_fold = KFold(n_splits=nrof_folds,shuffle=False)
  #nrof_thresholds
  val = np.zeros(nrof_folds)
  far=np.zeros(nrof_folds)
  indices = np.arange(nrof_pairs)
  
  for fold_idx, (train_set,test_set) in enumerate(k_fold.split(indices)):
   #Find the threshold that gives FAR = far_target
    far_train = np.zeros(nrof_thresholds)
    for threshold_idx, threshold in enumerate(thresholds):
      _, far_train[threshold_idx] = calculate_val_far(threshold, distances[train_set], labels[train_set])
    if np.max(far_train)>=far_target:
      f = interpolate.interp1d(far_train, thresholds, kind='slinear')
      threshold = f(far_target)
    else:
      threshold = 0.0

    val[fold_idx], far[fold_idx] = calculate_val_far(threshold, distances[test_set], labels[test_set])

  val_mean = np.mean(val)
  far_mean = np.mean(far)
  val_std = np.std(val)
  return val_mean, val_std, far_mean

def evaluate(distances,labels, nrof_folds=5):
  thresholds = np.arange(0,30,0.01)
  tpr,fpr,accuracy,pres = calculate_roc(thresholds,distances,labels,nrof_folds=nrof_folds)
  thresholds= np.arange(0,30,0.001)
  val,val_std,far =calculate_val(thresholds,distances,labels,1e-3,nrof_folds=nrof_folds)
  return tpr,fpr,accuracy,val,val_std,far,pres

def calculate_val_far(threshold, dist, actual_issame):
  predict_issame = np.less(dist, threshold)
  true_accept = np.sum(np.logical_and(predict_issame, actual_issame))
  false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))
  n_same = np.sum(actual_issame)
  n_diff = np.sum(np.logical_not(actual_issame))
  if n_diff == 0:
      n_diff = 1
  if n_same == 0:
      return 0,0
  val = float(true_accept) / float(n_same)
  far = float(false_accept) / float(n_diff)
  return val, far

def plot_roc(fpr,tpr,figure_name="roc.png"):
  import matplotlib.pyplot as plt
  plt.switch_backend('Agg')

  from sklearn.metrics import roc_curve, auc
  roc_auc = auc(fpr, tpr)
  fig = plt.figure()
  lw = 2
  plt.plot(fpr, tpr, color='red',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
  plt.plot([0, 1], [0, 1], color='blue', lw=lw, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver operating characteristic')
  plt.legend(loc="lower right")
  fig.savefig(figure_name, dpi=fig.dpi)

def plot_pre_recall(tpr,pres,figure_name="ap.png"):
  import matplotlib.pyplot as plt
  from inspect import signature
  
  step_kwargs = ({'step': 'post'}
               if 'step' in signature(plt.fill_between).parameters
               else {})
  plt.switch_backend('Agg')

  from sklearn.metrics import average_precision_score
  fig =plt.figure()
  plt.step(tpr,pres,color='b',alpha=0.2,where='post')
  plt.fill_between(tpr,pres,alpha=0.2,color='b',**step_kwargs)
  
  plt.xlabel('recall')
  plt.ylabel('precision')
  plt.title('AP')
  plt.legend(loc="lower right")
  fig.savefig(figure_name, dpi=fig.dpi)

def train_valid(model, optimizer, scheduler, epoch, dataloaders, data_size):
  for phase in ['train', 'valid']:
    labels, distances = [], []
    triplet_loss_sum  = 0.0
    if phase == 'train':
      scheduler.step()
      model.train()
    else:
      model.eval()
    for batch_idx, batch_sample in enumerate(dataloaders[phase]):
      anc_img = batch_sample['anc_img'].to(device)
      pos_img = batch_sample['pos_img'].to(device)
      neg_img = batch_sample['neg_img'].to(device)
      
      pos_cls = batch_sample['pos_class'].to(device)
      neg_cls = batch_sample['neg_class'].to(device)
      with torch.set_grad_enabled(phase == 'train'):
        # anc_embed, pos_embed and neg_embed are encoding(embedding) of image
        anc_embed, pos_embed, neg_embed = model(anc_img), model(pos_img), model(neg_img)

        # choose the hard negatives only for "training"
        pos_dist = l2_dist.forward(anc_embed, pos_embed)
        neg_dist = l2_dist.forward(anc_embed, neg_embed)

        all = (neg_dist - pos_dist < 2.0).cpu().numpy().flatten()
        if phase == 'train':
          hard_triplets = np.where(all == 1)
          if len(hard_triplets[0]) == 0:
            continue
        else:
          hard_triplets = np.where(all >= 0)
          

        anc_hard_embed = anc_embed[hard_triplets].to(device)
        pos_hard_embed = pos_embed[hard_triplets].to(device)
        neg_hard_embed = neg_embed[hard_triplets].to(device)

        anc_hard_img   = anc_img[hard_triplets].to(device)
        pos_hard_img   = pos_img[hard_triplets].to(device)
        neg_hard_img   = neg_img[hard_triplets].to(device)

        pos_hard_cls   = pos_cls[hard_triplets].to(device)
        neg_hard_cls   = neg_cls[hard_triplets].to(device)

        anc_img_pred   = model.forward_classifier(anc_hard_img).to(device)
        pos_img_pred   = model.forward_classifier(pos_hard_img).to(device)
        neg_img_pred   = model.forward_classifier(neg_hard_img).to(device)

        triplet_loss   = TripletLoss(2.0).forward(anc_hard_embed, pos_hard_embed, neg_hard_embed).to(device) #margin =2.0

        if phase == 'train':
          optimizer.zero_grad()
          triplet_loss.backward()
          optimizer.step()

        dists = l2_dist.forward(anc_embed, pos_embed)
        distances.append(dists.data.cpu().numpy())
        labels.append(np.ones(dists.size(0))) 

        dists = l2_dist.forward(anc_embed, neg_embed)
        distances.append(dists.data.cpu().numpy())
        labels.append(np.zeros(dists.size(0)))

        triplet_loss_sum += triplet_loss.item()

    avg_triplet_loss = triplet_loss_sum / data_size[phase]
    labels           = np.array([sublabel for label in labels for sublabel in label])
    distances        = np.array([subdist for dist in distances for subdist in dist])

    tpr, fpr, accuracy, val, val_std, far,pres = evaluate(distances, labels)
    
    print('  {} set - Triplet Loss       = {:.8f}'.format(phase, avg_triplet_loss))
    print('  {} set - Accuracy           = {:.8f}'.format(phase, np.mean(accuracy)))

    with open('/content/drive/My Drive/ColabNotebooks/log/{}_log_epoch{}.txt'.format(phase, epoch), 'w') as f:
      f.write(str(epoch) + '\t' + str(np.mean(accuracy)) + '\t' + str(avg_triplet_loss))

    if phase == 'train':
      torch.save({'epoch': epoch,'state_dict': model.state_dict()},'/content/drive/My Drive/ColabNotebooks/log/checkpoint_epoch{}.pth'.format(epoch))
    else:
      plot_roc(fpr, tpr, figure_name = '/content/drive/My Drive/ColabNotebooks/log/roc_valid_epoch_{}.png'.format(epoch))
      plot_pre_recall(tpr,pres, figure_name ='/content/drive/My Drive/ColabNotebooks/log/auc_valid_epoch_{}.png'.format(epoch))

def write_to_csv(root_path,csv_file_name):
  df = pd.DataFrame()
  list_train = os.listdir(root_path)
  for author in list_train:
    image_path = root_path+os.sep+author
    image_list = os.listdir(image_path)
    for image in image_list:
      #print(image)
      letter_id = image.split('.')[0]
      df =df.append({'id': letter_id, 'name': author},ignore_index=True)
  df = df.sort_values(by=['name','id']).reset_index(drop=True)
  df['class']=pd.factorize(df['name'])[0]
  df.to_csv(csv_file_name,index=False)

train_path = "/content/drive/My Drive/ColabNotebooks/DataProcessed/final_data_80_20/training_a/"
test_path = "/content/drive/My Drive/ColabNotebooks/DataProcessed/final_data_80_20/testing_a/"
train_csv_path = "/content/drive/My Drive/ColabNotebooks/DataProcessed/train.csv"
test_csv_path = "/content/drive/My Drive/ColabNotebooks/DataProcessed/test.csv"

write_to_csv(train_path,train_csv_path)
write_to_csv(test_path,test_csv_path)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
l2_dist =PairwiseDistance(2)
start_epoch = 0

def run_experiment(train_path,test_path,train_csv_path,test_csv_path):
  num_c = len(os.listdir(train_path))
  num_test_classes= len(os.listdir(test_path))
  num_test_triplets = num_test_classes
  num_train_triplets = num_c
  model = LetterNetModel(embedding_size=128,num_classes=num_c).to(device)
  optimizer = optim.Adam(model.parameters(),lr=0.001)
  scheduler = lr_scheduler.StepLR(optimizer,step_size=5,gamma =0.1)
  
  for epoch in range(0,20):
    print(80*'=')
    print ('Epoch [{}/{}]'.format(epoch,20-1))
    #def get_dataloader(train_root_dir,valid_root_dir,train_csv_name,valid_csv_name,num_train_triplets,num_valid_triplets,batch_size,num_workers)
    data_loader,data_size = get_dataloader(train_path,test_path,train_csv_path,test_csv_path,num_train_triplets,num_test_triplets,5,6)
    #print(data_size)
    train_valid(model,optimizer,scheduler,epoch,data_loader,data_size)
  print(80*"=")

#call run_experiment
run_experiment(train_path,test_path,train_csv_path,test_csv_path)

#sumAP =0.0
#for i in range(len(actual_list)):
#  sumAP+=ml_metrics.apk(actual_list[i].tolist(),predicted_list[i].tolist(),5)
#  print(ml_metrics.apk(actual_list[i].tolist(),predicted_list[i].tolist(),5))
#print("mean Average precision ",str(sumAP/len(actual_list)))
#print(actual_list)
#print(predicted_list)

num_epochs=20

fig =plt.figure()
for phase in ['train', 'valid']:
  list_epoch    = []
  list_loss     = []
  for epoch in range(num_epochs):
    with open('/content/drive/My Drive/ColabNotebooks/log/{}_log_epoch{}.txt'.format(phase, epoch), 'r') as f:
      reader = csv.reader(f, delimiter = '\t')
      d = list(reader)
      list_epoch.append(float(d[0][0]))
      list_loss.append(float(d[0][2]))
        
  if phase == 'train':
    plt.plot(list_epoch, list_loss, color = 'red')
  else:
    plt.plot(list_epoch, list_loss, color = 'blue')
  plt.xlabel('Epoch', fontsize = 15)
  plt.ylabel('Loss', fontsize = 15)
  plt.ylim(0, 0.01)
    
plt.savefig('/content/drive/My Drive/ColabNotebooks/log/loss.jpg', dpi = fig.dpi)

for phase in ['train', 'valid']:
  list_epoch    = []
  list_accuracy = []
  for epoch in range(num_epochs):
    with open('/content/drive/My Drive/ColabNotebooks/log/{}_log_epoch{}.txt'.format(phase, epoch), 'r') as f:
      reader = csv.reader(f, delimiter = '\t')
      d = list(reader)
      list_epoch.append(float(d[0][0]))
      list_accuracy.append(float(d[0][1]))
        
  if phase == 'train':
    plt.plot(list_epoch, list_accuracy, color = 'red', label = 'Train: Set')
  else:
    plt.plot(list_epoch, list_accuracy, color = 'blue', label = 'Valid: Set')

  plt.xlabel('Epoch', fontsize = 15)
  plt.ylabel('Accuracy', fontsize = 14)
  plt.ylim(0.55, 1)
    
plt.legend(loc='upper left')
plt.savefig('/content/drive/My Drive/ColabNotebooks/log/accuracy.jpg', dpi = fig.dpi)